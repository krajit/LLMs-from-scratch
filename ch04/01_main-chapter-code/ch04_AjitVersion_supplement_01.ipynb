{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see how the GPT2 we constucted compares with GPT on Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GPT2LMHeadModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "torch.Size([1, 10])\n",
      "tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]])\n",
      "attention_mask\n",
      "torch.Size([1, 10])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "for k in encoded_input:\n",
    "    print(k)\n",
    "    print(encoded_input[k].shape)\n",
    "    print(encoded_input[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "tensor([[[ -28.7086,  -27.9282,  -30.2393,  ...,  -37.6027,  -35.8095,\n",
      "           -28.8439],\n",
      "         [ -60.0999,  -57.8383,  -62.0975,  ...,  -68.0317,  -67.7715,\n",
      "           -60.5209],\n",
      "         [ -76.6621,  -78.3350,  -83.4316,  ...,  -89.8897,  -89.1758,\n",
      "           -81.1512],\n",
      "         ...,\n",
      "         [-145.1963, -145.4156, -150.1035,  ..., -153.6665, -150.1051,\n",
      "          -146.9225],\n",
      "         [ -84.2475,  -85.6010,  -91.2381,  ...,  -99.5435,  -97.8361,\n",
      "           -88.2612],\n",
      "         [-122.2600, -121.1204, -121.5651,  ..., -131.8222, -131.5997,\n",
      "          -115.1341]]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(encoded_input['input_ids'])\n",
    "\n",
    "print(output.logits.shape)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  13,  262,  351,  257, 1724,   11,  765,  588,   13,  198]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = output.logits\n",
    "probs = torch.softmax(logits,dim=-1)\n",
    "out_ids = torch.argmax(probs, dim=-1)\n",
    "out_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. the with a means, want like.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text = tokenizer.decode(out_ids.squeeze(0).tolist())\n",
    "out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to code. But often I have to get up from chair.  I hate that.  I hate that I have to do this.  I hate that I have to do this.  I hate that I have to do this.\n"
     ]
    }
   ],
   "source": [
    "# shortcut generate function\n",
    "text = \"I love to code. But often I have to get up from chair.  I hate that.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(**encoded_input, max_length=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
